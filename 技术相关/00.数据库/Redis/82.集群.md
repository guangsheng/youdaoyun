集群需要处理的几个关键点：数据分区、路由、节点通信、集群伸缩、故障转移  还有 运维

### 1.使用限制
- 目前只支持具有相同slot值的key执行批量操作，如mget、mset等。
- 只支持多key在同一个节点上的事务操作，当多个key分布在不同的节点上时无法使用事务功能。
	- redis提供了一种将不同键映射到同一个slot上的方法。
- 集群模式下只能使用一个数据库空间，即db 0.
- 复制结构支持一主多从，但只支持一层，不支持嵌套树状复制结构。

### 2.分区策略
常见的分区策略有节点取余分区和一致性哈希分区。Redis采用的是改进的一致性哈希分区，虚拟槽分区。  
 
一致性哈希分区的实现思路是为系统中每个节点分配一个token，范围一般在0~2^32，这些token构成一个哈希环。数据读写执行节点查找操作时，先根据key计算hash值，然后顺时针找到第一个大于等于该哈希值得token节点。  

一致性哈希分区有个缺点：在只有少量节点时，节点变化将大范围影响哈希环中数据映射，且在在增减节点时需要增加一倍或减少一半的节点才能保证数据和负载的均衡。  

虚拟槽就是在节点和数据之间增加了一级映射，Redis Cluster的槽范围是0~16383.将槽作为集群内数据管理和迁移的基本单位。每个节点都存储一定数量的槽。  
slot = CRC(key)&16383

#### hash_tag
Redis在计算槽时，如果键内容包含{和}大括号字符，则计算槽的有效部分是括号内的内容，否则采用键的全部内容计算。也就是说如下两个键对应的槽是一样的。  

```
set city:{wuhan}:1 test1
set city:{wuhan}:2 test2 
```

### 3.集群通信协议和路由
#### 3.1.通信协议
- redis集群采用P2P的Gossip协议进行节点通信。
- Gossip协议原理：节点彼此间不断通信交换信息，一段时间后所有的节点都会知道集群完整的信息，这种方式类似于流言传播。
- 每个节点在固定周期（每秒10次）内通过特定规则选择几个节点发送ping消息，接收到ping消息的节点用pong消息作为响应。

##### 消息分类
- ping : 每个节点每秒向多个其他节点发送ping消息，用于检测节点是否在线和交换彼此状态信息。ping消息中不仅封装了自身节点的状态数据，还封装了其他节点的状态数据。
- pong : ping和meet消息的响应。内部封装了自身的状态消息。
- meet : 用于通知新节点加入，消息发送者通知接收者加入到当前集群。
- fail : 当节点判定集群内另一个节点下线时，会向集群内广播一个fail消息。

##### 如何选择要发送消息的节点
每秒随机5次找出最久没有通信的节点 + 最后通信时间大于node-timeout/2的节点

##### ping消息体大小
消息头（2K左右）+消息体  
消息体大小与集群内节点数有关，消息体中包含自身节点信息和十分之一其他节点信息。

##### 配置纪元
配置纪元是一个只增不减的整数，每个主节点自身维护一个配置纪元（cofigEpoch）来标示当前主节点的版本，所有主节点的配置纪元都不相同。  
从节点会复制主节点的配置纪元。  
整个集群维护了一个全局的配置纪元用于记录集群内所有主节点配置纪元的最大版本。  

配置纪元出现冲突的解决方法：nodeID更大的一方递增全局配置纪元并赋值给当前节点来区分冲突。  

主要作用：  

- 用于记录关键事件（每次关键事件都会更新配置纪元）
- 主节点具有更大的配置纪元代表了更新的集群状态，当节点进行ping/pong消息交换出现slots等关键信息不一致时，以配置纪元更大的一方为准。

#### 3.2.路由  
需要客户端和服务端配合才能完成请求的路由。  
路由处理需要考虑异常场景：如数据迁移，迁移了一半等。  

##### 重定向
如果key是在本节点，获取数据返回；如果请求的是key并不在本节点，会向客户端返回重定向信息，信息中会包含这个key实际所在的节点IP和PORT。  
也可以通过集群命令一次性获取所有槽位与节点的对应关系。    
重定向错误分为两类：MOVED和ASK。 ASK说明集群正在进行槽数据迁移，客户端应该在本次请求中做临时重定向，不需要更新本地槽缓存。MOVED重定向说明槽已经明确指定给了另一个节点，需要更新本地槽缓存。



##### 注意事项
- 用户在选择Smart客户端时建议review下集群交换代码：异常判定和重试逻辑，更新槽的并发控制

### 4.搭建集群
- 搭建流程： 准备节点（启动节点）--节点握手--分配槽位   对外提供服务
- 通常的架构是多个节点组成集群，且每个节点都要做成高可用的，例如一主一从。
- **节点ID**: 用于标识集群内的一个节点。不同于运行ID，节点ID在集群初始化时只创建一次，节点重启时会加载集群配置文件进行重用。也是一个40位16进制字符串。

#### 4.1.准备节点
- 开启 cluster-enable
- 设置集群内配置文件（相对于非集群模式增加的配置文件）：cluster-config-file
- redis自动维护集群配置文件，指定集群配置文件名称即可，不需要手动修改。
- 在添加节点、节点下线、故障转移等发生时，节点会自动将集群状态保存到集群配置文件中。
- 建议各节点配置文件名称用端口号区分。

典型配置文件示例（redis-6379.conf)

```
port 6379
logfile "/Users/shiguangsheng/user_program/redis_cluster/log/redis-6379.log"
pidfile "/Users/shiguangsheng/user_program/redis_cluster/redis-6379.pid"
dir "/Users/shiguangsheng/user_program/redis_cluster/data"
dbfilename "dump-6379.rdb"
appendfilename "appendonly-6379.rdb"
daemonize yes
loglevel debug
#集群相关
cluster-enabled yes
cluster-node-timeout 150000
cluster-config-file "nodes-6379.conf"
```

#### 4.2.节点握手
握手： 指一批运行在集群模式下的节点通过Gossip协议彼此通信，达到感知对方的过程。  

握手命令，由客户端发起：```cluster meet {ip} {port}```  

确认握手情况，执行：```cluster nodes```和```cluster info```

**说明：**我们只需要在任一节点上执行cluster meet命令加入新节点即可，因为握手状态会通过集群通信方式在集群内传播，其他节点可以通过这个消息自动发现新节点并发起握手流程。  

握手后，集群还不能工作，因为集群还不知道应该将数据放到哪个节点，数据应该在哪个节点查询。还需要给节点分配槽位。  

#### 4.3.分配槽
命令介绍(在客户端执行)：  

```
#分配槽位
cluster add slots {0...5461}
#指定一个从节点
cluster replicate {nodeId(节点ID)}
```

### 5.扩容
准备新节点-加入集群（握手，meet消息）-迁移槽和数据-添加从节点  

- 新节点刚开始都是主节点状态，但是由于没有分配槽，并不能提供服务。
- 如果保证新加入节点后，各个节点还是负责相似数量的槽位：每个节点都将其负责的一部分槽位赋予新节点即可。

#### 迁移数据
数据迁移是逐个槽进行的  

##### 迁移数据流程  
1. 对目标节点发送```cluster setslot {slot} importing {sourceNodeId}```命令，让目标节点**准备**导入槽的数据。
2. 对源节点发送```cluster setslot {slot} migrating {targetNodeId}```命令，让源节点**准备**迁出槽的数据。
3. 源节点循环执行```cluster getkeysinslot {slot} {count}```命令，获取count个属于槽{slot}的键
4. 在源节点执行```migrate {targetIp} {targetPort} "" - {timeout} keys {keys ...}```命令，把获取的键通过pipeline机制批量迁移到目标节点。
5. 重复3和4直到迁移完槽{slot}的数据
6. 向集群内所有主节点发送```cluster setslot {slot} node {targetNodeId}```命令，通知槽{slot}已经分配给目标节点。

#### 添加从节点
使用```cluster replicate {masterNodeId}```为主节点添加对应从节点。

**添加过程中，从节点除了对主节点发起全量复制外还需要更新本地节点的集群相关状态。**

### 6.缩容
- 迁移槽和数据: 和扩容步骤类似，相反。
- 节点下线

#### 节点下线（忘记节点）
```cluster forget {downNodeId}```  

当节点接受到forget命令后，会把NodeId指定的节点加入到禁用列表中，在禁用列表中的节点不再发送Gossip消息，禁用列表的有效期是60秒。也就是说，我们有60秒的时间让集群内所有节点忘记下线节点。  

线上不建议直接使用forget命令，因为会造成大量节点命令交互。建议使用```redis-trib.rb del-node {host:port} {downNodeId}```命令  

当下线主节点有从节点时需要把从节点指向到其他主节点（隐式的包含了全量复制），因此，对于主从节点都需要下线的情况，建议先下线从节点。


### 7.故障发现和迁移
- 主观下线：我认为你有问题
- 客观下线：超过一半的兄弟都认为你有问题

#### 故障发现
- 和哨兵机制类似，是通过消息传播机制实现的。  
- 如果在cluster-node-timeout时间内通信一直失败，则发送节点认为接受节点故障。
- 半数以上持有槽的主节点都标记某个节点是故障的后，就认为这个节点真正故障了。
- 向集群广播一条fail消息，通知集群内所有节点标记故障节点为客观下线状态并立刻生效，并通知故障节点的从节点触发故障转移流程。

##### 具体实现
每个节点会维护一个下线（故障）报告链表，下线报告中保存了报告故障的节点结构和最近收到的下线报告时间。下线报告来源于自身的判断和接受的ping信息中携带的其他节点（ping信息中包含发送ping信息节点的状态信息和集群中十分之一的其他节点的状态信息）状态。  
每个下线报告都存在有效期。有效期限是server.cluster_node_timeout*2.  
即如果有效期限内无法收集到一半以上槽节点的下线报告，那么之前的下线报告就会失效。也就是说主观下线上报的速度追不上下线报告过期的速度，那么故障节点将永远无法被标记为客观下线而导致故障转移失败。

#### 故障恢复
1. 从故障的节点的从节点中选出一个节点用于替换他
2. 替换主节点

##### 如何选择从节点
- 从节点与主节点断线时间不能超过 cluster-node-time*cluster-slave-validity-factor
- 通过复制偏移量确定优先级，复制偏移量越大（也就是主从差异越小）的优先级越高。

##### 选择后还需要
- 更新配置纪元
- 向集群内广播选举信息
- 集群内持有槽的主节点进行投票，超过一半节点同意后这个从节点才真正的可以开始替换主节点

##### 替换流程
- 当前从节点取消复制变为主节点
- 执行clusterDelSlot操作撤销故障主节点负责的槽位，并执行clusterAddSlot操作将这些槽位委派给自己
- 向集群广播自己的pong消息进行通知

#### 故障转移时间
falover-time <= cluster-node-timeout + cluster-node-timeout/2 + 1000ms

#### 手动故障迁移
在主节点需要迁移到另一台机器、自动故障转移失败（如主节点超过一半故障）等场景时会用到手动迁移功能。  
几个手动迁移命令的说明和区别：  

- cluster failover : 会先停掉客户端与主节点的连接并是主从达到完全一致才真正发起状态切换。
- cluster failover force : 用于主节点宕机的场景，此时可能会丢失数据。
- cluster failover takeover : 用于集群内超过一半主节点故障的场景，可能会丢失数据和导致配置纪元冲突。

注意事项：迁移完成后，旧主节点变为从节点后，会向新的主节点发起全量复制流程。

### 8.注意事项
- 扩容： 如果我们手动执行cluster meet命令加入已经存在于其他集群的节点，会造成被加入的节点的集群合并到现有集群的情况（流言传播），从而造成数据丢失和错误，因此要尽量避免。
- 当持有槽的主节点下线时，从故障发现到自动完成转移期间整个集群是不可用状态，对于大多数业务这个是不可容忍的。因此建议将参数```cluster-require-full-coverage```配置为no，当主节点故障时，只影响它负责槽的相关命令执行。
- 处于带宽消耗的的考虑，官方建议集群最大规模在1000左右。
- 在集群模式下内部实现对publish命令都会向所有节点进行广播，对带宽的影响较大，要慎用。
- 集群模式下从节点默认不接受任何读写请求，打开后需要考虑如下问题：复制延迟、读取过期数据、从节点故障灯。
- 集群模式下不建议开启读写分离，如果要扩展集群性能，建议直接扩展主节点数量。

#### 集群倾斜问题
可以从如下几个方面考虑：  

- 节点和槽位是否分配均匀
- 不同槽对应的键数量差异多大
- 集合对象是否包含大量元素（同一个键对应的值占用空间的差别可能是非常大的）

建议：
 
- 不要对热键使用hash_tag,会造成压力集中到某一个节点。
- 对于一致性要求不高的场景，客户端可以使用本地缓存来减少热键的调用


### 9.小技巧
#### 常用命令
- cluster nodes
- cluster info
- cluster slaves {nodeID} :返回指定ID下所有从节点信息

#### 数据迁移工具
唯品会开发的 redis-migrate-tool(https://github.com/vipshop/redis-migrate-tool)

### 10.redis-trib.rb(在redis源码的src目录下）
#### 搭建集群
1. 准备节点（即将需要放入集群的节点一一启动）
2. 执行```redis-trib.rb create --replicas num {ip:port} [{ip:port} ...]```命令创建集群（完成握手和槽分配过程），num表示为集群中每个主节点配置几个从节点
3. 执行```redis-trib.rb check```命令做完整性检查

```
#1
$ redis-server redis-6379.conf
$ redis-server redis-6380.conf
$ redis-server redis-6381.conf
$ redis-server redis-6382.conf
$ redis-server redis-6383.conf
$ redis-server redis-6384.conf
$ ps -ef |grep redis |grep -v grep
  501 10950     1   0  4:08下午 ??         0:01.80 redis-server *:6379 [cluster]
  501 14315     1   0  4:47下午 ??         0:00.03 redis-server *:6380 [cluster]
  501 14317     1   0  4:47下午 ??         0:00.03 redis-server *:6381 [cluster]
  501 14319     1   0  4:47下午 ??         0:00.03 redis-server *:6382 [cluster]
  501 14321     1   0  4:47下午 ??         0:00.03 redis-server *:6383 [cluster]
  501 14323     1   0  4:47下午 ??         0:00.03 redis-server *:6384 [cluster]

#2
$ redis-trib.rb create --replicas 1 127.0.0.1:6379 127.0.0.1:6380 127.0.0.1:6381 127.0.0.1:6382 127.0.0.1:6383 127.0.0.1:6384
>>> Creating cluster
>>> Performing hash slots allocation on 6 nodes...
Using 3 masters:
127.0.0.1:6379
127.0.0.1:6380
127.0.0.1:6381
Adding replica 127.0.0.1:6382 to 127.0.0.1:6379
Adding replica 127.0.0.1:6383 to 127.0.0.1:6380
Adding replica 127.0.0.1:6384 to 127.0.0.1:6381
M: c34d437f761db461732fdbab7d80ad9b5f702b38 127.0.0.1:6379
   slots:0-5460 (5461 slots) master
M: 19afe77e5bf7caff12a32b7eea43f73dc0124338 127.0.0.1:6380
   slots:5461-10922 (5462 slots) master
M: a665a4461d7ba114c124cc7de332474387378503 127.0.0.1:6381
   slots:10923-16383 (5461 slots) master
S: 5867b6c57c39a2727fead93a06e4e32face0ba3b 127.0.0.1:6382
   replicates c34d437f761db461732fdbab7d80ad9b5f702b38
S: 047de377c413b8ed4c2d0ef84b80dc108f6354bb 127.0.0.1:6383
   replicates 19afe77e5bf7caff12a32b7eea43f73dc0124338
S: 4ec63f9f6b2e0b28ca9d682c8577c6cc3b295b02 127.0.0.1:6384
   replicates a665a4461d7ba114c124cc7de332474387378503
Can I set the above configuration? (type 'yes' to accept): yes
>>> Nodes configuration updated
>>> Assign a different config epoch to each node
>>> Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join......
>>> Performing Cluster Check (using node 127.0.0.1:6379)
M: c34d437f761db461732fdbab7d80ad9b5f702b38 127.0.0.1:6379
   slots:0-5460 (5461 slots) master
M: 19afe77e5bf7caff12a32b7eea43f73dc0124338 127.0.0.1:6380
   slots:5461-10922 (5462 slots) master
M: a665a4461d7ba114c124cc7de332474387378503 127.0.0.1:6381
   slots:10923-16383 (5461 slots) master
M: 5867b6c57c39a2727fead93a06e4e32face0ba3b 127.0.0.1:6382
   slots: (0 slots) master
   replicates c34d437f761db461732fdbab7d80ad9b5f702b38
M: 047de377c413b8ed4c2d0ef84b80dc108f6354bb 127.0.0.1:6383
   slots: (0 slots) master
   replicates 19afe77e5bf7caff12a32b7eea43f73dc0124338
M: 4ec63f9f6b2e0b28ca9d682c8577c6cc3b295b02 127.0.0.1:6384
   slots: (0 slots) master
   replicates a665a4461d7ba114c124cc7de332474387378503
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.

#3
$ redis-trib.rb check 127.0.0.1:6379
>>> Performing Cluster Check (using node 127.0.0.1:6379)
M: c34d437f761db461732fdbab7d80ad9b5f702b38 127.0.0.1:6379
   slots:0-5460 (5461 slots) master
   1 additional replica(s)
S: 4ec63f9f6b2e0b28ca9d682c8577c6cc3b295b02 127.0.0.1:6384
   slots: (0 slots) slave
   replicates a665a4461d7ba114c124cc7de332474387378503
S: 5867b6c57c39a2727fead93a06e4e32face0ba3b 127.0.0.1:6382
   slots: (0 slots) slave
   replicates c34d437f761db461732fdbab7d80ad9b5f702b38
M: a665a4461d7ba114c124cc7de332474387378503 127.0.0.1:6381
   slots:10923-16383 (5461 slots) master
   1 additional replica(s)
M: 19afe77e5bf7caff12a32b7eea43f73dc0124338 127.0.0.1:6380
   slots:5461-10922 (5462 slots) master
   1 additional replica(s)
S: 047de377c413b8ed4c2d0ef84b80dc108f6354bb 127.0.0.1:6383
   slots: (0 slots) slave
   replicates 19afe77e5bf7caff12a32b7eea43f73dc0124338
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.

$ redis-cli
127.0.0.1:6379> cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:6
cluster_my_epoch:1
cluster_stats_messages_sent:70
cluster_stats_messages_received:70
127.0.0.1:6379> cluster nodes
4ec63f9f6b2e0b28ca9d682c8577c6cc3b295b02 127.0.0.1:6384 slave a665a4461d7ba114c124cc7de332474387378503 0 1509267591934 6 connected
5867b6c57c39a2727fead93a06e4e32face0ba3b 127.0.0.1:6382 slave c34d437f761db461732fdbab7d80ad9b5f702b38 0 1509267593946 4 connected
c34d437f761db461732fdbab7d80ad9b5f702b38 127.0.0.1:6379 myself,master - 0 0 1 connected 0-5460
a665a4461d7ba114c124cc7de332474387378503 127.0.0.1:6381 master - 0 1509267594954 3 connected 10923-16383
19afe77e5bf7caff12a32b7eea43f73dc0124338 127.0.0.1:6380 master - 0 1509267590925 2 connected 5461-10922
047de377c413b8ed4c2d0ef84b80dc108f6354bb 127.0.0.1:6383 slave 19afe77e5bf7caff12a32b7eea43f73dc0124338 0 1509267595963 5 connected
```

#### 扩容
- 准备新节点
- 为集群添加新节点:```redis-trib.rb add-node new_host:new_port existing_host:existing_port --slave --master-id <arg>```
- 迁移槽和数据：```redis-trib.rb reshard host:port --from <arg> --to <arg> --slots <arg> --yes --timeout <arg> --pipeline <arg>```
	-  host:port: 集群内任一节点地址
	-  --form: 源节点ID，如果有多个源节点，使用逗号分隔，如果是全部源节点，不用填写
	-  --to: 目标节点ID，只能填写一个
	-  --slots : 需要迁移的槽的总数量
	-  --yes: 当打印出reshard的执行计划时，是否需要用户输入yes确认
	-  --timeout：migrate的超市时间，默认60000毫秒。
	-  --pipeline: 每次批量迁移键的数量，默认为10.
- 添加从节点

```
#1
$ redis-server redis-6385.conf
$ redis-server redis-6386.conf

#2
$ redis-trib.rb add-node 127.0.0.1:6385 127.0.0.1:6379
>>> Adding node 127.0.0.1:6385 to cluster 127.0.0.1:6379
>>> Performing Cluster Check (using node 127.0.0.1:6379)
...
[OK] All 16384 slots covered.
>>> Send CLUSTER MEET to node 127.0.0.1:6385 to make it join the cluster.
[OK] New node added correctly.
$ redis-trib.rb add-node 127.0.0.1:6386 127.0.0.1:6379
>>> Adding node 127.0.0.1:6386 to cluster 127.0.0.1:6379
>>> Performing Cluster Check (using node 127.0.0.1:6379)
...
[OK] All 16384 slots covered.
>>> Send CLUSTER MEET to node 127.0.0.1:6386 to make it join the cluster.
[OK] New node added correctly.

#3.
$ redis-trib.rb reshard 127.0.0.1:6379
>>> Performing Cluster Check (using node 127.0.0.1:6379)
...
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
How many slots do you want to move (from 1 to 16384)? 4096
What is the receiving node ID?
...
Moving slot 1362 from 127.0.0.1:6379 to 127.0.0.1:6385:
Moving slot 1363 from 127.0.0.1:6379 to 127.0.0.1:6385:
Moving slot 1364 from 127.0.0.1:6379 to 127.0.0.1:6385:

#4
$ redis-cli -p 6386 cluster replicate 5a4f0f2c77dca8a139e4b30b8ed744769e2cbd41
OK

#5查看
$ redis-cli cluster nodes
4ec63f9f6b2e0b28ca9d682c8577c6cc3b295b02 127.0.0.1:6384 slave a665a4461d7ba114c124cc7de332474387378503 0 1509270281611 6 connected
a0d3a5d55515188e43f902ad20f2e3c8ecae3e19 127.0.0.1:6386 slave 5a4f0f2c77dca8a139e4b30b8ed744769e2cbd41 0 1509270280608 7 connected
5867b6c57c39a2727fead93a06e4e32face0ba3b 127.0.0.1:6382 slave c34d437f761db461732fdbab7d80ad9b5f702b38 0 1509270278588 4 connected
c34d437f761db461732fdbab7d80ad9b5f702b38 127.0.0.1:6379 myself,master - 0 0 1 connected 1365-5460
a665a4461d7ba114c124cc7de332474387378503 127.0.0.1:6381 master - 0 1509270276574 3 connected 12288-16383
19afe77e5bf7caff12a32b7eea43f73dc0124338 127.0.0.1:6380 master - 0 1509270279598 2 connected 6827-10922
047de377c413b8ed4c2d0ef84b80dc108f6354bb 127.0.0.1:6383 slave 19afe77e5bf7caff12a32b7eea43f73dc0124338 0 1509270277578 5 connected
5a4f0f2c77dca8a139e4b30b8ed744769e2cbd41 127.0.0.1:6385 master - 0 1509270267497 7 connected 0-1364 5461-6826 10923-12287
```


#### 缩容
- 下线从节点
- 迁移数据
- 下线主节点

```
#1
$ redis-trib.rb del-node 127.0.0.1:6382 5867b6c57c39a2727fead93a06e4e32face0ba3b
>>> Removing node 5867b6c57c39a2727fead93a06e4e32face0ba3b from cluster 127.0.0.1:6382
>>> Sending CLUSTER FORGET messages to the cluster...
>>> SHUTDOWN the node.

#2
$ redis-trib.rb reshard 127.0.0.1:6379
>>> Performing Cluster Check (using node 127.0.0.1:6379)
M: c34d437f761db461732fdbab7d80ad9b5f702b38 127.0.0.1:6379
   slots:1365-5460 (4096 slots) master
   1 additional replica(s)
...
How many slots do you want to move (from 1 to 16384)? 1365
What is the receiving node ID? a665a4461d7ba114c124cc7de332474387378503
Please enter all the source node IDs.
  Type 'all' to use all the nodes as source nodes for the hash slots.
  Type 'done' once you entered all the source nodes IDs.
Source node #1:c34d437f761db461732fdbab7d80ad9b5f702b38
Source node #2:done
...
Moving slot 4094 from 127.0.0.1:6379 to 127.0.0.1:6380:

重复执行

#3
$ redis-trib.rb del-node 127.0.0.1:6379 c34d437f761db461732fdbab7d80ad9b5f702b38
>>> Removing node c34d437f761db461732fdbab7d80ad9b5f702b38 from cluster 127.0.0.1:6379
>>> Sending CLUSTER FORGET messages to the cluster...
>>> SHUTDOWN the node.

#4查看
$ redis-cli -p 6380 cluster nodes
4ec63f9f6b2e0b28ca9d682c8577c6cc3b295b02 127.0.0.1:6384 slave a665a4461d7ba114c124cc7de332474387378503 0 1509270950596 8 connected
a0d3a5d55515188e43f902ad20f2e3c8ecae3e19 127.0.0.1:6386 slave 5a4f0f2c77dca8a139e4b30b8ed744769e2cbd41 0 1509270949586 10 connected
047de377c413b8ed4c2d0ef84b80dc108f6354bb 127.0.0.1:6383 slave 19afe77e5bf7caff12a32b7eea43f73dc0124338 0 1509270945554 9 connected
a665a4461d7ba114c124cc7de332474387378503 127.0.0.1:6381 master - 0 1509270948577 8 connected 1365-2729 12288-16383
19afe77e5bf7caff12a32b7eea43f73dc0124338 127.0.0.1:6380 myself,master - 0 0 9 connected 2730-4094 6827-10922
5a4f0f2c77dca8a139e4b30b8ed744769e2cbd41 127.0.0.1:6385 master - 0 1509270947568 10 connected 0-1364 4095-6826 10923-12287
```