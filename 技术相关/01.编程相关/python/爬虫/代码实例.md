#### 爬取京东某个网页的图片（正则写的不是太好）

```python
import re
import urllib.request

def craw(url, page):
    html_1=urllib.request.urlopen(url).read()
    html_1=str(html_1)
    #print(html_1)
    pat_1='<div id="plist".+?<div class="page clearfix">'
    result_1=re.compile(pat_1).findall(html_1)
    result_1=result_1[0]
    #???print(result_1)
    #pat_2='img width="220" height="220" data-img="1" src="//(.+?\.jpg)"'
    pat_2='src="//(.+?\.jpg)"'
    imagelist=re.compile(pat_2).findall(result_1)
    pat_2='data-lazy-img="//(.+?\.jpg)"'
    imagelist_tmp=re.compile(pat_2).findall(result_1)
    imagelist.extend(imagelist_tmp)
    x=1
    for imageurl in imagelist:
    	imagename="/Users/shiguangsheng/Desktop/MyZone/KM/Code/test_code/python_test/image/"+str(page)+str(x)+".jpg"
    	imageurl="http://"+imageurl
    	print(imageurl)
    	try:
    		urllib.request.urlretrieve(imageurl, filename=imagename)
    	except urllib.error.URLError as e:
    		if hasattr(e, "code"):
    		    x+=1
    		    print(e.code)
    		if hasattr(e, "reason"):
    			x+=1
    			print(e.reason)
    	x+=1

url="https://list.jd.com/list.html?cat=9987,653,655&page=2"
craw(url, 1)
```

#### 爬取糗事百科
```python
# -*- coding:utf-8 -*-
import re
import urllib.request

def getcontent(url, page):
	headers=("User-Agent", "Mozilla/5.0 (Windows NT 6.1; Win64; x64) \
        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.81 Safari/537.36")
	opener = urllib.request.build_opener()
	opener.addheaders = [headers]
	urllib.request.install_opener(opener)
	data=urllib.request.urlopen(url).read().decode("utf-8")

	userpat='target="_blank" title="(.*?)">'
	contentpat='<div class="content">(.*?)</div>'
	userlist=re.compile(userpat, re.S).findall(data)
	contentlist=re.compile(contentpat, re.S).findall(data)
	
	x=1
	for content in contentlist:
		content=content.replace("\n","")
		#用字符串作为变量名，先将对应的字符串赋给一个变量
		name="content"+str(x)
		#通过exec函数实现用字符串作为变量名并赋值
		exec(name+'=content')
		x+=1

	y=1
	for user in userlist:
		name="content"+str(y)
		print("用户 "+str(page)+str(y)+"是："+user)
		print("内容是：")
		exec("print("+name+")")
		print("\n")
		y+=1

for i in range(1, 30):
	url="http://www.qiushibaike.com/8hr/page/"+str(i)
	getcontent(url,i)
```

#### 爬取csdn的链接
```python
# -*- coding:utf-8 -*-
import re
import urllib.request

def getlink(url):
	headers=("User-Agent", "Mozilla/5.0 (Windows NT 6.1; Win64; x64) \
        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.81 Safari/537.36")
	opener = urllib.request.build_opener()
	opener.addheaders = [headers]
	urllib.request.install_opener(opener)
	data=urllib.request.urlopen(url).read().decode("utf-8")
	pat='(https?://[^\s)";]+\.(\w|/)*)'
	link=re.compile(pat).findall(data)
	#去重
	link=list(set(link))
	
	return link

url="http://blog.csdn.net/"
linklist=getlink(url)
for link in linklist:
	print(link[0])
```